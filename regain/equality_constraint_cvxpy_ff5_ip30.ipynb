{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import multivariate_normal as mvnorm\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
    "                               AutoMinorLocator)\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "%matplotlib inline\n",
    "\n",
    "import pdb, time\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardised Fama French 5 to industry portfolio 30\n",
    "# Import data\n",
    "data = pd.read_pickle(\"/Users/sikai/Downloads/ff5_30_standard_4000_draws.pkl\")\n",
    "\n",
    "# # Restrict to 100 time points\n",
    "X = data[10:110].transpose(2, 1, 0)\n",
    "X_cov = np.einsum('ijkl,jmkl->imkl', np.expand_dims(X, 1), np.expand_dims(X, 0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Synthetic\n",
    "# Import data\n",
    "data = pd.read_pickle(\"~/Downloads/integrated_draws_20_5_4.pkl\")\n",
    "n_draws, n_time, dim_X, dim_fac = data['beta'].shape\n",
    "beta = data['beta']\n",
    "X_sd = data['x_sd']\n",
    "\n",
    "# Generate X\n",
    "m_fac = np.zeros((dim_fac))\n",
    "s_fac = np.diag(np.ones((dim_fac)))\n",
    "fac = np.expand_dims(mvnorm(m_fac, s_fac, (n_draws, n_time)), -1)\n",
    "\n",
    "X = np.matmul(beta, fac).squeeze()\n",
    "\n",
    "m_eps = np.zeros((dim_X))\n",
    "for i in range(n_draws):\n",
    "    eps = mvnorm(m_eps, np.diag(X_sd[i] ** 2), n_time)\n",
    "    X[i] += X[i] + eps\n",
    "    \n",
    "X = X.transpose((2, 0, 1))\n",
    "X_cov = np.einsum('ijkl,jmkl->imkl', np.expand_dims(X, 1), np.expand_dims(X, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50940.731002451\n",
      "50436.82788163196\n",
      "50147.1613527367\n",
      "49864.375888182825\n",
      "49586.494015220305\n",
      "49313.311848285215\n",
      "> /Users/sikai/Dropbox/Research/post_sum/code/regain/regain/covariance/taylor_prox_time_graphical_lasso.py(210)taylor_prox_time_graphical_lasso()\n",
      "-> out = minimize_scalar(_f, bounds=(0., 1.),  method='bounded')\n",
      "(Pdb) loss_res_old\n",
      "array([-9.3717846 , -9.638149  , -8.64117192, -9.26541924, -8.6514556 ,\n",
      "       -9.03053568, -9.25489833, -8.64921378, -7.2610666 , -9.01845534,\n",
      "       -8.94094379, -8.65606155, -8.74005592, -2.24329533, -8.94959412,\n",
      "       -8.07963251, -8.54841906, -7.35693192, -8.01902318, -7.99483055,\n",
      "       -8.18189401, -8.11137519, -7.49965806, -7.54180107, -1.93433225,\n",
      "       -8.22070013, -8.16493439, -5.97981023, -7.69589528, -7.81657825,\n",
      "       -7.44855534, -8.17211468,  0.59630586, -7.58596549, -7.53970131,\n",
      "       -8.17685528, -7.14992736, -6.43755232, -5.14704921, -7.82877871,\n",
      "       -7.67627213, -7.19584666, -7.6068428 , -6.29577857, -7.19522866,\n",
      "       -7.33069349, -7.19568412, -4.6140125 , -6.18481494, -7.4760715 ,\n",
      "       -7.09789177, -7.23401986, -7.22529985, -7.58351818, -6.71775273,\n",
      "       -7.28317243, -6.96686645, -6.39153696, -7.20415555, -7.27350235,\n",
      "       -7.39373101, -6.6101565 , -6.2043617 , -6.9045165 , -7.3161517 ,\n",
      "       -7.21861534, -7.31115681, -6.25557452, -5.22397449, -6.5709426 ,\n",
      "       -7.20387017, -2.28966792, -4.4576091 , -4.99215859, -4.2033738 ,\n",
      "       -6.88548669, -5.78421045, -6.85575585, -6.76288162, -4.86163242,\n",
      "       -7.00355779, -7.37537018, -7.08443128, -7.39902644, -6.56314133,\n",
      "       -7.36859429, -6.87534345, -6.73552263, -6.98122567, -6.31769936,\n",
      "       -7.063627  , -5.25336508, -6.16326219, -6.78696674, -7.00421482,\n",
      "       -7.23471615, -6.95173747, -6.85379012, -7.00922582, -6.39460848])\n",
      "(Pdb) n\n",
      "> /Users/sikai/Dropbox/Research/post_sum/code/regain/regain/covariance/taylor_prox_time_graphical_lasso.py(212)taylor_prox_time_graphical_lasso()\n",
      "-> Z_0 = _Z_0(out.x)\n",
      "(Pdb) n\n",
      "> /Users/sikai/Dropbox/Research/post_sum/code/regain/regain/covariance/taylor_prox_time_graphical_lasso.py(213)taylor_prox_time_graphical_lasso()\n",
      "-> print(out.fun, out.x)\n",
      "(Pdb) out.fun\n",
      "inf\n",
      "(Pdb) out = minimize_scalar(_f)  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sikai/.pyenv/versions/3.7.6/lib/python3.7/site-packages/scipy/optimize/optimize.py:2116: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  tmp2 = (x - v) * (fx - fw)\n",
      "/Users/sikai/.pyenv/versions/3.7.6/lib/python3.7/site-packages/scipy/optimize/optimize.py:2117: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  p = (x - v) * tmp2 - (x - w) * tmp1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Pdb) print(out.fun, out.x)\n",
      "49.39289933680952 1.5036634767457785e-05\n"
     ]
    }
   ],
   "source": [
    "from regain.covariance import TaylorProxTimeGraphicalLasso, TimeGraphicalLasso\n",
    "tic = time.perf_counter()\n",
    "tgl = TaylorProxTimeGraphicalLasso(max_iter=3000, loss='LL', c_level=0.2, rho=1e2, theta=0.5, tol=1e-4, rtol=1e-4, psi=\"laplacian\")\n",
    "emp_inv_score_tp, baseline_score_tp, fit_score_tp, pre_tp = tgl.fit_cov(X_cov).eval_cov_pre() \n",
    "toc = time.perf_counter()\n",
    "print('Running Time :{}'.format(toc - tic))\n",
    "min_pre_tp = np.amin(np.abs(pre_tp[pre_tp != 0]))\n",
    "print(min_pre_tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([sum(sum(abs(pre_tp[t]) == 0)) for t in range(X_cov.shape[-1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum([sum(sum(abs(pre_tp[t]) < 1e-4)) for t in range(X_cov.shape[-1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from regain.covariance import InequalityTimeGraphicalLasso, TimeGraphicalLasso\n",
    "res = {}\n",
    "for i in [1.]:\n",
    "    tic = time.perf_counter()\n",
    "    tgl = InequalityTimeGraphicalLasso(max_iter=1000, loss='LL', c_level=0.2, c_prox='cvx', rho=1e2, div=i, tol=1e-8, rtol=1e-8, psi=\"laplacian\")\n",
    "    res[i] = tgl.fit_cov(X_cov).eval_cov_pre() \n",
    "    toc = time.perf_counter()\n",
    "    print('Running Time :{}'.format(toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regain.covariance import InequalityTimeGraphicalLasso, TimeGraphicalLasso\n",
    "tic = time.perf_counter()\n",
    "tgl = InequalityTimeGraphicalLasso(max_iter=5000, loss='LL', c_level=0.2, c_prox='grad', rho=2e2, theta=0.5, psi=\"laplacian\")\n",
    "emp_inv_score_grad, baseline_score_grad, fit_score_grad, pre_grad = tgl.fit_cov(X_cov).eval_cov_pre() \n",
    "toc = time.perf_counter()\n",
    "print('Running Time :{}'.format(toc - tic))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from regain.covariance import CVXInequalityTimeGraphicalLasso, TimeGraphicalLasso\n",
    "tic = time.perf_counter()\n",
    "tgl = CVXInequalityTimeGraphicalLasso(max_iter=1e4, loss='LL', c_level=0.2, theta=0.5, psi=\"laplacian\", tol=1e-6)\n",
    "emp_inv_score, baseline_score_gradore, fit_score, pre_cvx = tgl.fit_cov(X_cov).eval_cov_pre() \n",
    "toc = time.perf_counter()\n",
    "print('Running Time :{}'.format(toc - tic))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.save(\"mosek_sol_ff5_30_standard_alpha_0.2.npy\", pre_cvx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regain.covariance import CVXInequalityTimeGraphicalLasso, TimeGraphicalLasso\n",
    "pre_cvx = np.load(\"mosek_sol_ff5_30_standard_alpha_0.2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regain.norm import l1_od_norm\n",
    "from regain.validation import check_norm_prox\n",
    "psi, prox_psi, psi_node_penalty = check_norm_prox(tgl.psi)\n",
    "\n",
    "def penalty_objective(Z_0, Z_1, Z_2, psi):\n",
    "    \"\"\"Penalty-only objective function for time-varying graphical LASSO.\"\"\"\n",
    "    return sum(map(l1_od_norm, Z_0)) + sum(map(psi, Z_2 - Z_1))\n",
    "\n",
    "pre = {}\n",
    "fit_score = {}\n",
    "for i in [1e-2, 1e-3, 1e-4, 1e-6]:\n",
    "# for i in [min_pre_tp]:\n",
    "    pre[i] = np.array([k * (np.abs(k) >= i) for k in pre_cvx])\n",
    "    tgl.precision_ = pre[i]\n",
    "    emp_inv_score, baseline_score, fit_score[i], _ = tgl.eval_cov_pre() \n",
    "    print(penalty_objective(pre[i], pre[i][:-1], pre[i][:1], psi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgl = TimeGraphicalLasso(alpha=1., beta=1., mode='admm', rho=1, tol=1e-4,\n",
    "            rtol=1e-4, psi='laplacian', max_iter=3000, verbose=False, assume_centered=False, \n",
    "            return_history=False, update_rho_options=None, compute_objective=True, \n",
    "            stop_at=None, stop_when=1e-4, suppress_warn_list=False, init='empirical')\n",
    "fit_score_, pre_ = tgl.fit_cov(X_cov).eval_cov_pre()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(fit_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "fig.patch.set_facecolor('white')\n",
    "colors = \"rgm\"\n",
    "color_index = 0\n",
    "\n",
    "idx = 50\n",
    "\n",
    "ax.plot(range(X_cov.shape[0] * X_cov.shape[0]), tgl.emp_inv[idx].flatten(), color='k', label=r'Empirical Inverse')\n",
    "for i in [min_pre_tp]:\n",
    "# for i in [1e-2, 1e-3, 1e-4]:\n",
    "    ax.plot(range(X_cov.shape[0] * X_cov.shape[0]), pre[i][idx].flatten(), color='r', alpha=0.5, \n",
    "            label=r'Constraint TGL SCS, Thres = {}'.format(i))\n",
    "# for i in [2.]:    \n",
    "#     ax.plot(range(X_cov.shape[0] * X_cov.shape[0]), res[i][3][idx].flatten(), color='g', alpha=0.5,\n",
    "#             label=r'Constraint TGL ADMM CVX, Div = {}'.format(i))\n",
    "ax.plot(range(X_cov.shape[0] * X_cov.shape[0]), pre_grad[idx].flatten(), color='m', alpha=0.5,\n",
    "        label=r'Constraint TGL ADMM Gradient')\n",
    "ax.plot(range(X_cov.shape[0] * X_cov.shape[0]), pre_tp[idx].flatten(), color='g', alpha=0.5,\n",
    "        label=r'Constraint TGL ADMM Linear')\n",
    "ax.plot(range(X_cov.shape[0] * X_cov.shape[0]), pre_[idx].flatten(), color='y', label=r'Vanilla TGL')\n",
    "\n",
    "\n",
    "fig.legend(fontsize=15)\n",
    "ax.set_ylabel('Values', fontsize=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax.set_xlabel('Entries', fontsize=15)\n",
    "ax.set_title(r'Precisions at t={} for Empirical Inverse, TGL and Constraint TGL SCS/ADMM'.format(idx), fontsize=20)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('pre_{}_{}_{}_admm_relax_taylor.pdf'.format(idx, dim_X, dim_fac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "fig.patch.set_facecolor('white')\n",
    "colors = \"rgm\"\n",
    "color_index = 0\n",
    "\n",
    "for i in [min_pre_tp]:\n",
    "# for i in [1e-2, 1e-3, 1e-4]:\n",
    "    diff = (pre[i] -  pre_cvx).flatten()\n",
    "    ax.hist(diff, bins=100, color='r', alpha=0.5, \n",
    "            label=r'Constraint TGL SCS, Thres = {}'.format(i))\n",
    "# for i in [2.]:    \n",
    "#     diff = (res[i][3] -  pre_cvx).flatten()\n",
    "#     ax.hist(diff, bins=50, color='g', alpha=0.2,\n",
    "#             label=r'Constraint TGL ADMM CVX, Div = {}'.format(i))\n",
    "diff = (pre_grad -  pre_cvx).flatten()\n",
    "ax.hist(diff, bins=100, color='m', alpha=0.2,\n",
    "        label=r'Constraint TGL ADMM Gradient')\n",
    "diff = (pre_tp -  pre_cvx).flatten()\n",
    "ax.hist(diff, bins=100, color='g', alpha=0.2,\n",
    "        label=r'Constraint TGL ADMM Linear')\n",
    "# diff = (pre_ -  pre_cvx).flatten()\n",
    "# ax.hist(diff, bins=100, color='y', alpha=0.5, \n",
    "#         label=r'Vanilla TGL')\n",
    "\n",
    "fig.legend(fontsize=15)\n",
    "ax.set_ylabel('Counts', fontsize=15)\n",
    "ax.set_xlabel('Values', fontsize=15)\n",
    "major_loc = MultipleLocator(base=0.1)\n",
    "ax.xaxis.set_major_locator(major_loc)\n",
    "ax.set_xlim((-1, 1))\n",
    "# ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax.set_title(r'Difference in Precisions for TGL and Constraint TGL SCS/ADMM', fontsize=20)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('pre_diff_hist_{}_{}_taylor.pdf'.format(dim_X, dim_fac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "fig.patch.set_facecolor('white')\n",
    "colors = \"rgb\"\n",
    "color_index = 0\n",
    "plt.rcParams[\"axes.prop_cycle\"] = plt.cycler(\"color\", plt.cm.tab20.colors)\n",
    "\n",
    "ax.plot(range(X_cov.shape[-1]), res[1][0], color='k', label=r'Empirical Inverse')\n",
    "ax.plot(range(X_cov.shape[-1]), res[1][1], color='c', label=r'Constraint')\n",
    "# for i in [min_pre_tp]:\n",
    "for i in [1e-2, 1e-3, 1e-4]:\n",
    "    mean_diff = np.mean(np.array(fit_score[i]) - baseline_score)\n",
    "#     ax.plot(range(X_cov.shape[-1]), fit_score[i], color='r', alpha=0.5, \n",
    "    ax.plot(range(X_cov.shape[-1]), fit_score[i], color=colors[color_index], alpha=0.5, \n",
    "            label=r'Constraint TGL SCS, Thres = {}, Mean Diff = {:.3f}'.format(i, mean_diff))\n",
    "    color_index += 1\n",
    "# # for i in [2.]:    \n",
    "# #     mean_diff = np.mean(np.array(res[i][2]) - res[i][1])\n",
    "# #     ax.plot(range(X_cov.shape[-1]), res[i][2], alpha=0.5, color='g',\n",
    "# #             label=r'Constraint TGL ADMM CVX, Div = {}, Mean Diff = {:.3f}'.format(i, mean_diff))\n",
    "# # mean_diff = np.mean(np.array(fit_score_diff) - baseline_score)\n",
    "# # ax.plot(range(n_time), fit_score_diff, alpha=0.5,\n",
    "# #         label=r'Constraint TGL ADMM Difference, Mean Diff = {:.3f}'.format(mean_diff))\n",
    "mean_diff = np.mean(np.array(fit_score_grad) - baseline_score_grad)\n",
    "ax.plot(range(X_cov.shape[-1]), fit_score_grad, alpha=0.5, color='m',\n",
    "        label=r'Constraint TGL ADMM Gradient, Mean Diff = {:.3f}'.format(mean_diff))\n",
    "# mean_diff = np.mean(np.array(fit_score_tp) - baseline_score_tp)\n",
    "# print(np.sum((np.array(fit_score_tp) - baseline_score_tp) > 0.01))\n",
    "# ax.plot(range(X_cov.shape[-1]), fit_score_tp, alpha=0.5, color='g',\n",
    "#         label=r'Constraint TGL ADMM Linear, Mean Diff = {:.3f}'.format(mean_diff))\n",
    "# mean_diff = np.mean(np.array(fit_score_) - res[1][1])\n",
    "# print(np.sum((np.array(fit_score_) - res[1][1]) > 0.01))\n",
    "# ax.plot(range(X_cov.shape[-1]), fit_score_, color='y', label=r'Vanilla TGL, Mean Diff = {:.3f}'.format(mean_diff))\n",
    "\n",
    "fig.legend(fontsize=15)\n",
    "ax.set_ylabel('Negative Log Likelihood', fontsize=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax.set_xlabel('Time t', fontsize=15)\n",
    "ax.set_title(r'Negative Log Likelihood for Empirical Inverse, Constraint, TGL and Constraint TGL SCS/ADMM', fontsize=20)\n",
    "# ax.set_title(r'Negative Log Likelihood for Empirical Inverse, Constraint, TGL and Constraint TGL SCS', fontsize=20)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('diff_like_{}_{}_admm_taylor.pdf'.format(dim_X, dim_fac))\n",
    "# plt.savefig('diff_like_{}_{}_scs.pdf'.format(dim_X, dim_fac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "fig.patch.set_facecolor('white')\n",
    "colors = \"rgb\"\n",
    "color_index = 0\n",
    "\n",
    "ax.plot(range(X_cov.shape[-1]), [sum(sum(abs(tgl.emp_inv[k]) > 0)) for k in range(X_cov.shape[-1])], \n",
    "        color='k', label=r'Empirical Inverse')\n",
    "# for i in [min_pre_tp]:\n",
    "for i in [1e-2, 1e-3, 1e-4]:\n",
    "    supp = [sum(sum(abs(pre[i][t]) > 0)) for t in range(X_cov.shape[-1])]\n",
    "    mean_supp = np.mean(supp)\n",
    "#     ax.plot(range(X_cov.shape[-1]), supp, color='r', alpha=0.5, \n",
    "    ax.plot(range(X_cov.shape[-1]), supp, color=colors[color_index], alpha=0.5, \n",
    "            label=r'Constraint TGL SCS, Thres = {}, Mean Supp = {}'.format(i, mean_supp))\n",
    "    color_index += 1\n",
    "# # for i in [2.]:    \n",
    "# #     supp = [sum(sum(abs(res[i][3][t]) > 0)) for t in range(X_cov.shape[-1])]\n",
    "# #     mean_supp = np.mean(supp)\n",
    "# #     ax.plot(range(X_cov.shape[-1]), supp, color='g', alpha=0.5,\n",
    "# #             label=r'Constraint TGL ADMM CVX, Div = {}, Mean Supp = {:.3f}'.format(i, mean_supp))\n",
    "supp = [sum(sum(abs(pre_grad[t]) > 0)) for t in range(X_cov.shape[-1])]\n",
    "ax.plot(range(X_cov.shape[-1]), supp, color='m', alpha=0.5,\n",
    "        label=r'Constraint TGL ADMM Gradient, Mean Supp = {:.3f}'.format(np.mean(supp)))\n",
    "# supp = [sum(sum(abs(pre_tp[t]) > 0)) for t in range(X_cov.shape[-1])]\n",
    "# ax.plot(range(X_cov.shape[-1]), supp, color='g', alpha=0.5,\n",
    "#         label=r'Constraint TGL ADMM Linear, Mean Supp = {:.3f}'.format(np.mean(supp)))\n",
    "supp = [sum(sum(abs(pre_[t]) > 0)) for t in range(X_cov.shape[-1])]\n",
    "ax.plot(range(X_cov.shape[-1]), [sum(sum(abs(pre_[k]) > 0)) for k in range(X_cov.shape[-1])], \n",
    "        color='y', label=r'Vanilla TGL, , Mean Supp = {:.3f}'.format(np.mean(supp)))\n",
    "\n",
    "\n",
    "fig.legend(fontsize=15)\n",
    "ax.set_ylabel('Support', fontsize=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax.set_xlabel('Time t', fontsize=15)\n",
    "ax.set_title(r'Support for Empirical Inverse, TGL and Constraint TGL SCS/ADMM', fontsize=20)\n",
    "# ax.set_title(r'Support for Empirical Inverse, TGL and Constraint TGL SCS', fontsize=20)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('diff_supp_{}_{}_admm_taylor.pdf'.format(dim_X, dim_fac))\n",
    "# plt.savefig('diff_supp_{}_{}_scs.pdf'.format(dim_X, dim_fac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "fig.patch.set_facecolor('white')\n",
    "colors = \"rgb\"\n",
    "color_index = 0\n",
    "\n",
    "diff_emp = [norm(tgl.emp_inv[t] - tgl.emp_inv[t-1], 'fro') for t in range(1, X_cov.shape[-1])]\n",
    "ax.plot(range(1, X_cov.shape[-1]), np.array(diff_emp), alpha=1, color='k', label=r'Empirical Inverse')\n",
    "# for i in [min_pre_tp]:\n",
    "for i in [1e-2, 1e-3, 1e-4]:\n",
    "    diff = [norm(pre[i][t] - pre[i][t-1], 'fro') for t in range(1, X_cov.shape[-1])]\n",
    "    mean_diff = np.mean(diff)\n",
    "#     ax.plot(range(1, X_cov.shape[-1]), diff, color='r', alpha=0.5, \n",
    "    ax.plot(range(1, X_cov.shape[-1]), diff, color=colors[color_index], alpha=0.5, \n",
    "            label=r'Constraint TGL SCS, Thres = {}, Mean Diff = {:.3f}'.format(i, mean_diff))\n",
    "    color_index += 1\n",
    "# # for i in [2.]:    \n",
    "# #     diff_cvx = [norm(res[i][3][t] - res[i][3][t-1], 'fro') for t in range(1, X_cov.shape[-1])]\n",
    "# #     ax.plot(range(1, X_cov.shape[-1]), diff_cvx, color='g', alpha=0.5,\n",
    "# #             label=r'Constraint TGL ADMM CVX, Div = {}, Mean Diff = {:.3f}'.format(i, np.mean(diff_cvx)))\n",
    "diff_grad = [norm(pre_grad[t] - pre_grad[t-1], 'fro') for t in range(1, X_cov.shape[-1])]\n",
    "ax.plot(range(1, X_cov.shape[-1]), diff_grad, color='m', alpha=0.5,\n",
    "        label=r'Constraint TGL ADMM Gradient, Mean Diff = {:.3f}'.format(np.mean(diff_grad)))\n",
    "# diff_grad = [norm(pre_tp[t] - pre_tp[t-1], 'fro') for t in range(1, X_cov.shape[-1])]\n",
    "# ax.plot(range(1, X_cov.shape[-1]), diff_grad, color='g', alpha=0.5,\n",
    "#         label=r'Constraint TGL ADMM Linear, Mean Diff = {:.3f}'.format(np.mean(diff_grad)))\n",
    "diff_fit_ = [norm(pre_[t] - pre_[t-1], 'fro') for t in range(1, X_cov.shape[-1])]\n",
    "ax.plot(range(1, X_cov.shape[-1]), np.array(diff_fit_), color='y', alpha=1, \n",
    "        label=r'Vanilla TGL, Mean Diff = {:.3f}'.format(np.mean(diff_fit_)))\n",
    "\n",
    "        \n",
    "fig.legend(fontsize=15)\n",
    "ax.set_ylabel('Difference in Frobenius Norm', fontsize=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "ax.set_xlabel('Time t', fontsize=15)\n",
    "ax.set_title(r'Difference in Frobenius Norm for Empirical Inverse, TGL and Constraint TGL SCS/ADMM', fontsize=20)\n",
    "# ax.set_title(r'Difference in Frobenius Norm for Empirical Inverse, TGL and Constraint TGL SCS', fontsize=20)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('diff_fro_{}_{}_admm_taylor.pdf'.format(dim_X, dim_fac))\n",
    "# plt.savefig('diff_fro_{}_{}_scs.pdf'.format(dim_X, dim_fac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
