{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LVGLASSO comparison\n",
    "This notebook shows a comparison between LGL and LTGL with respect to LVGLASSO, the implementation of the original model (for a single time point) presented here: https://ieeexplore.ieee.org/abstract/document/5707106\n",
    "\n",
    "We compare with the code of the authors of https://arxiv.org/pdf/1206.1275, that reimplemented the method using ADMM, a minimisation procedure which we adopted as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.covariance import empirical_covariance\n",
    "\n",
    "from regain import datasets\n",
    "from regain.covariance import latent_graphical_lasso_, latent_time_graphical_lasso_\n",
    "\n",
    "# config\n",
    "np.random.seed(0)\n",
    "n_samples = 100\n",
    "n_dim_obs = 10\n",
    "n_dim_lat = 2\n",
    "T = 8\n",
    "tau = 0.1\n",
    "alpha = 0.1\n",
    "\n",
    "dataset = datasets.make_dataset(\n",
    "    n_samples=n_samples, n_dim_lat=n_dim_lat, update_ell='fixed',\n",
    "    update_theta='l2', normalize_starting_matrices=True, n_dim_obs=n_dim_obs,\n",
    "    T=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Single timestamp\n",
    "Check if, with only one timestamp, the method behave the same as Chandrasekaran/Ma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_cov = empirical_covariance(dataset.data[0], assume_centered=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Do we behave as ourselves with the same functional as Ma?\n",
    "The following is the latent time graphical model inference with only one covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_time = latent_time_graphical_lasso_.latent_time_graphical_lasso(\n",
    "    emp_cov[None, ...], alpha=alpha, tau=tau, tol=1e-5, rtol=1e-5,\n",
    "    rho=1. / emp_cov.shape[0], verbose=0, max_iter=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare it with the code for the latent graphical model inference (without time). <br>\n",
    "Since there is only one covariance matrix, we expect to obtain the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_static = latent_graphical_lasso_.latent_graphical_lasso(\n",
    "    emp_cov, alpha=alpha, tau=tau, tol=1e-5, rtol=1e-5,\n",
    "    rho=1. / emp_cov.shape[0], verbose=0, max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(\n",
    "    [np.allclose(x, y) for x, y in zip(results_static, results_time)])\n",
    "assert np.linalg.matrix_rank(results_static[1]) == np.linalg.matrix_rank(\n",
    "    results_time[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional (requires Matlab/Octave installed)\n",
    "Now we check if the result is the same as the LVGLASSO Matlab algorithm. To do that, we implemented a simple wrapper which rely on `matlab.engine` or `oct2py`, to run Matlab code directly from Python. For `matlab.engine`, It requires Matlab 2016 or higher installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regain.wrapper import lvglasso\n",
    "from sklearn.datasets.base import Bunch\n",
    "\n",
    "result = lvglasso(emp_cov, alpha, tau, 1. / emp_cov.shape[0])\n",
    "ma_output = Bunch(**result)\n",
    "\n",
    "assert np.all(\n",
    "    [\n",
    "        np.allclose(x, y, atol=1e-3)\n",
    "        for x, y in zip(results_static[:2], (ma_output.S, ma_output.L))\n",
    "    ])\n",
    "assert np.linalg.matrix_rank(ma_output.L) == np.linalg.matrix_rank(\n",
    "    results_time[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Time-varying vs separate for each time\n",
    "This is to justify the choice of the additional penalties which constrain subsequent matrices in time to behave similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "n_samples = 100\n",
    "n_dim_obs = 10\n",
    "n_dim_lat = 2\n",
    "T = 8\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "dataset = datasets.make_dataset(\n",
    "    n_samples=n_samples, n_dim_lat=n_dim_lat, update_ell='fixed',\n",
    "    update_theta='l2', normalize_starting_matrices=True, n_dim_obs=n_dim_obs,\n",
    "    T=T)\n",
    "\n",
    "X, y = dataset.X, dataset.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we check again if the results are the same with beta and eta is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_cov = np.array(\n",
    "    [\n",
    "        empirical_covariance(data, assume_centered=False)\n",
    "        for data in dataset.data\n",
    "    ])\n",
    "emp_list = np.array(emp_cov).transpose(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_time = latent_time_graphical_lasso_.latent_time_graphical_lasso(\n",
    "    emp_cov, alpha=alpha, tau=tau, tol=1e-5, rtol=1e-5,\n",
    "    rho=1. / emp_cov.shape[0], beta=0, eta=0, verbose=0, max_iter=500)\n",
    "\n",
    "results_static = [\n",
    "    latent_graphical_lasso_.latent_graphical_lasso(\n",
    "        x, alpha=alpha, tau=tau, tol=1e-5, rtol=1e-5,\n",
    "        rho=1. / emp_cov.shape[0], verbose=0, max_iter=500) for x in emp_cov\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(\n",
    "    [\n",
    "        np.allclose(results_static[i][0], results_time[0][i], atol=1e-2)\n",
    "        for i in range(T)\n",
    "    ])\n",
    "assert np.all(\n",
    "    [\n",
    "        np.linalg.matrix_rank(results_static[i][1]) == np.linalg.matrix_rank(\n",
    "            results_time[1][i]) for i in range(T)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, optional: it requires Matlab or Octave installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = lvglasso(emp_list, alpha, tau, rho=1. / emp_list[0].shape[0])\n",
    "ma_output = Bunch(**result)\n",
    "\n",
    "# note time is the last dim, as emp_list\n",
    "ma_output.R = np.array(ma_output.R.T)\n",
    "ma_output.S = np.array(ma_output.S.T)\n",
    "ma_output.L = np.array(ma_output.L.T)\n",
    "\n",
    "assert np.allclose(results_time[0], ma_output.R + ma_output.L, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(results_time[0] - (ma_output.R + ma_output.L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = np.array(ma_output.L)\n",
    "LL = results_time[1]\n",
    "\n",
    "ranks_ma = [np.linalg.matrix_rank(l) for l in L]\n",
    "ranks_ours = [np.linalg.matrix_rank(l) for l in LL]\n",
    "assert np.all(\n",
    "    [\n",
    "        np.linalg.matrix_rank(l) == np.linalg.matrix_rank(ll)\n",
    "        for l, ll in zip(L, LL)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penalty contribution\n",
    "\n",
    "We now checked that in the limit case of one time and in the case in which we do not consider the penalties that involve time we perform equivalentely. Now, with CV on the parameters on synhtetic data generated with norm2 we want to see if our method performs better than LVGLASSO applied on different time stamps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "from regain import utils\n",
    "from regain.covariance import LatentTimeGraphicalLasso\n",
    "\n",
    "ltgl = GridSearchCV(\n",
    "    LatentTimeGraphicalLasso(),\n",
    "    dict(\n",
    "        tau=np.logspace(-2, np.log(.5), 10),\n",
    "        alpha=np.logspace(-2, np.log(.5), 10)), cv=StratifiedShuffleSplit(10),\n",
    "    return_train_score=True).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_list = list(ltgl.best_estimator_.covariance_)\n",
    "alpha = ltgl.best_params_['alpha']\n",
    "tau = ltgl.best_params_['tau']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precs = []\n",
    "lats = []\n",
    "covss = []\n",
    "for data in dataset.data:\n",
    "    lgl = LatentGraphicalLasso(tau=tau, alpha=alpha,\n",
    "                               assume_centered=False).fit(X, y)\n",
    "    precs.append(lgl.precision_)\n",
    "    lats.append(lgl.latent_)\n",
    "    covss.append(lgl.covariance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error obtained w.r.t. the observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regain.utils import error_norm_time\n",
    "print(\n",
    "    \"Error norm time with observed precision: \\n LTGL: {:.3f}\\n LVGLASSO: {:.3f}\"\n",
    "    .format(\n",
    "        error_norm_time(\n",
    "            ltgl.best_estimator_.precision_ - ltgl.best_estimator_.latent_,\n",
    "            dataset.thetas_observed),\n",
    "        error_norm_time(\n",
    "            np.array(precs) - np.array(lats), dataset.thetas_observed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error obtained w.r.t. the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Error norm time of LTGL: \\n Precision: {:.3f}\\n Latent: {:.3f}\\n\".format(\n",
    "        error_norm_time(ltgl.best_estimator_.precision_, dataset.thetas),\n",
    "        error_norm_time(ltgl.best_estimator_.latent_, dataset.ells)))\n",
    "\n",
    "print(\n",
    "    \"Error norm time of LVGLASSO: \\n Precision: {:.3f}\\n Latent: {:.3f}\".\n",
    "    format(\n",
    "        error_norm_time(np.array(precs), dataset.thetas),\n",
    "        error_norm_time(np.array(lats), dataset.ells)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error in ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ltgl.best_estimator_.set_params(beta=1e12,eta=1e12).fit(data_grid)\n",
    "print(\n",
    "    \"Rank latent matrices LTGL: {}\".format(\n",
    "        [np.linalg.matrix_rank(i) for i in ltgl.best_estimator_.latent_]))\n",
    "print(\n",
    "    \"Rank latent matrices LVGLASSO: {}\".format(\n",
    "        [np.linalg.matrix_rank(i) for i in lats]))\n",
    "print(\n",
    "    \"Rank true latent matrices: {}\".format(\n",
    "        np.linalg.matrix_rank(dataset.ells)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error in structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regain.utils import structure_error\n",
    "\n",
    "print(\n",
    "    \"f1 score LTGL: {:.2f}\".format(\n",
    "        structure_error(\n",
    "            dataset.thetas, ltgl.best_estimator_.precision_, thresholding=1,\n",
    "            eps=1e-2)['f1']))\n",
    "print(\n",
    "    \"f1 score LVGLASSO: {:.2f}\".format(\n",
    "        structure_error(\n",
    "            dataset.thetas, np.array(precs), thresholding=1, eps=1e-2)['f1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "n_samples = 100\n",
    "# n_dim_obs = 10\n",
    "n_dim_lat = 2\n",
    "T = 10\n",
    "np.random.seed(0)\n",
    "import time\n",
    "results = {}\n",
    "\n",
    "for n_dim_obs in tqdm(np.logspace(3, 6, 10)):\n",
    "    dataset = datasets.make_dataset(\n",
    "        n_samples=n_samples, n_dim_lat=n_dim_lat, update_ell='fixed',\n",
    "        update_theta='l2', normalize_starting_matrices=True,\n",
    "        n_dim_obs=n_dim_obs, T=T)\n",
    "\n",
    "    X, y = dataset.X, dataset.y\n",
    "\n",
    "    emp_cov = np.array(\n",
    "        [\n",
    "            empirical_covariance(data, assume_centered=False)\n",
    "            for data in dataset.data\n",
    "        ])\n",
    "    emp_list = np.array(emp_cov).transpose(1, 2, 0)\n",
    "\n",
    "    res, elapsed_time = [], []\n",
    "    for x in emp_cov:\n",
    "        tic = time.time()\n",
    "        results_static = latent_graphical_lasso_.latent_graphical_lasso(\n",
    "            x, alpha=alpha, tau=tau, tol=1e-5, rtol=1e-5,\n",
    "            rho=1. / emp_cov.shape[0], verbose=0, max_iter=500)\n",
    "        tac = time.time() - tic\n",
    "\n",
    "        res.append(results_static)\n",
    "        elapsed_time.append(tac)\n",
    "\n",
    "    results[('lgl', n_dim_obs)] = dict(res=res, elapsed_time=elapsed_time)\n",
    "\n",
    "    res, elapsed_time = [], []\n",
    "    for x in emp_cov:\n",
    "        tic = time.time()\n",
    "        result = lvglasso(x, alpha, tau, 1. / x.shape[0])\n",
    "        ma_output = Bunch(**result)\n",
    "\n",
    "        ma_output.R = np.array(ma_output.R)\n",
    "        ma_output.S = np.array(ma_output.S)\n",
    "        ma_output.L = np.array(ma_output.L)\n",
    "        tac = ma_output.elapsed_time  #time.time() - tic\n",
    "\n",
    "        res.append(ma_output)\n",
    "        elapsed_time.append(tac)\n",
    "\n",
    "    results[('lvglasso',\n",
    "             n_dim_obs)] = dict(res=ma_output, elapsed_time=elapsed_time)\n",
    "#     results[('lvglasso', n_dim_obs)] = ma_output\n",
    "\n",
    "# assert np.allclose(results_time[0], ma_output.R + ma_output.L, atol=1e-3)\n",
    "\n",
    "#     L = np.array(ma_output.L)\n",
    "#     LL = results_time[1]\n",
    "\n",
    "#     ranks_ma = [np.linalg.matrix_rank(l)for l in L]\n",
    "#     ranks_ours = [np.linalg.matrix_rank(l)for l in LL]\n",
    "# assert np.all([np.linalg.matrix_rank(l) == np.linalg.matrix_rank(ll) for l, ll in zip(L, LL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regain.utils import flatten\n",
    "df = pd.DataFrame(\n",
    "    flatten(\n",
    "        [\n",
    "            [\n",
    "                (k[0], int(k[1] * k[1] - 1) // 2 * T, x)\n",
    "                for x in results[k]['elapsed_time']\n",
    "            ] for k in results\n",
    "        ]), columns=['method', 'n_unknowns', 'time'])\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "f, ax = plt.subplots()\n",
    "g = sns.pointplot(data=df, hue='method', x='n_unknowns', y='time')\n",
    "g.set_yscale(\"log\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
