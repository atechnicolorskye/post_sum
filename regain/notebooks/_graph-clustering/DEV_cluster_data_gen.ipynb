{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Dev notebook]\n",
    "## How does STGL work with TICC data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from imp import reload\n",
    "from time import time\n",
    "\n",
    "from sklearn.utils.extmath import squared_norm\n",
    "from sklearn.base import clone\n",
    "from sklearn.covariance import empirical_covariance, EmpiricalCovariance, GraphicalLasso\n",
    "from sklearn.cluster.hierarchical import AgglomerativeClustering\n",
    "from scipy import linalg\n",
    "from sklearn.metrics import v_measure_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.gaussian_process.kernels import RBF, ExpSineSquared\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "from regain.covariance import (time_graphical_lasso_, latent_time_graphical_lasso_,\n",
    "                               kernel_time_graphical_lasso_, kernel_latent_time_graphical_lasso_)\n",
    "\n",
    "from regain.datasets import kernels\n",
    "from regain.utils import structure_error, error_norm_time, normalize_matrix\n",
    "\n",
    "from ticc import TICC_solver\n",
    "reload(TICC_solver)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import local_utils\n",
    "reload(local_utils)\n",
    "from local_utils import *\n",
    "\n",
    "from regain.bayesian import wishart_process_, sampling, stats; reload(wishart_process_)\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# # print the inv matrix\n",
    "# res = sns.clustermap(data.inv, row_cluster=False, col_cluster=False)\n",
    "# ax = res.ax_heatmap\n",
    "# for i in range(n_samples * len(clusters)):\n",
    "#     ax.axvline((i + 1) * n_dim)\n",
    "#     ax.axhline((i + 1) * n_dim)\n",
    "\n",
    "# cov = []\n",
    "# for c in np.unique(y):\n",
    "#     idx = y == c\n",
    "#     cov.append(empirical_covariance(X[idx]))\n",
    "# cov = np.asarray(cov)\n",
    "\n",
    "# kernel = np.zeros((cov.shape[0], cov.shape[0]))\n",
    "# for i in range(cov.shape[0]):\n",
    "#     for j in range(i + 1, cov.shape[0]):\n",
    "#         kernel[i, j] = kernel[j, i] = (np.linalg.norm((cov[i]) - (cov[j])))\n",
    "\n",
    "# mm = np.sum(np.abs(kernel), axis=1)\n",
    "# kernel += np.eye(cov.shape[0]) * mm\n",
    "\n",
    "# normalize_matrix(kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "beta = 2\n",
    "eta = 4\n",
    "\n",
    "stgl = kernel_time_graphical_lasso_.SimilarityTimeGraphicalLasso(\n",
    "    alpha=alpha, beta=beta, max_iter=500, psi='l1',\n",
    "    max_iter_ext=10, eps=1e-1)\n",
    "\n",
    "sltgl = kernel_latent_time_graphical_lasso_.SimilarityLatentTimeGraphicalLasso(\n",
    "    alpha=alpha, tau=3, beta=beta, eta=eta, max_iter=500, psi='l1',\n",
    "    max_iter_ext=10, eps=1e-1)\n",
    "clust = AgglomerativeClustering(affinity='precomputed', linkage='complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# from skopt import searchcv; from imp import reload; reload(searchcv)\n",
    "# from skopt.space import Real, Categorical, Integer\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.svm import SVC \n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# domain = {'alpha': Real(1e-1, 1e0, prior='log-uniform'),\n",
    "#           'tau': Real(1e-1, 1e+1, prior='log-uniform'),\n",
    "#           'beta': Integer(1, 60)}\n",
    "    \n",
    "# cv = StratifiedShuffleSplit(10, test_size=0.2)\n",
    "    \n",
    "# bscv = searchcv.BayesSearchCV(sltgl, domain, n_iter=32, cv=cv)\n",
    "\n",
    "# bscv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "partitions = [5, 10, 50, 100, 500]\n",
    "n_splits = 5\n",
    "\n",
    "n_dim = 100\n",
    "w_size = 2\n",
    "n_samples = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (a) and (b) experiment\n",
    "shuffle = False\n",
    "\n",
    "np.random.seed(42)\n",
    "vs, ne, se = {}, {}, {}\n",
    "datas = {}\n",
    "for i in partitions:\n",
    "    for j in tqdm(range(n_splits)):\n",
    "        np.random.seed((i+j+3)*n_dim)\n",
    "        n_clusts = np.random.randint(10) + 5\n",
    "        data = kernels.make_cluster_representative(\n",
    "            n_clusters=n_clusts, n_dim=n_dim, n_samples=i, shuffle=shuffle)\n",
    "        X, y = data.X, data.y\n",
    "        n_times = np.unique(y).size\n",
    "        thetas_true = np.array([data.precs[l] for l in y])\n",
    "        thetas_true_sparse = np.array([data.sparse_precs[l] for l in y])\n",
    "        labels_true = data.id_cluster #[::(len(clusters) * i // n_times)]\n",
    "\n",
    "        \n",
    "        # WP + similarity\n",
    "        wp = wishart_process_.WishartProcess(kernel='ess', verbose=2, learn_ell=False,\n",
    "                                            n_iter=200).fit(X, y)\n",
    "        tic = time()\n",
    "        wp.fit(X, y)        \n",
    "        tac = time() - tic\n",
    "        tgl_prec_sims = kernel_time_graphical_lasso_.precision_similarity(wp.precision_, psi)\n",
    "        labels_pred = clust.set_params(n_clusters=np.unique(labels_true).size).fit_predict(tgl_prec_sims)\n",
    "        labels_pred = np.repeat(labels_pred, i)\n",
    "        mdl = wp\n",
    "        obs_precs = np.array([mdl.precision_[l] for l in y])\n",
    "        obs_precs_sparse = np.array([mdl.precision_[l] for l in y])\n",
    "        \n",
    "        set_results(vs, mdl, 'wp', i, labels_true, labels_pred, \n",
    "                    thetas_true_sparse, thetas_true, obs_precs_sparse, obs_precs, tac)\n",
    "        \n",
    "        \n",
    "        scales = np.unique(np.diff(np.where(data.cluster_series == 0)))\n",
    "        scales = scales[scales > 1]\n",
    "        kernel = RBF(1)\n",
    "        for s in scales:\n",
    "            kernel += ExpSineSquared(periodicity=s, length_scale=0.5) * 10\n",
    "        kern = kernel(np.arange(np.unique(y).size)[:, None])\n",
    "        \n",
    "        # KTGL + similarity\n",
    "        mdl = kernel_time_graphical_lasso_.KernelTimeGraphicalLasso(\n",
    "            alpha=1, max_iter=500, psi='l1', kernel=kern)\n",
    "        \n",
    "        tic = time()\n",
    "        mdl.fit(X, y)        \n",
    "        psi = kernel_time_graphical_lasso_.check_norm_prox(mdl.psi)[0]\n",
    "        tgl_prec_sims = kernel_time_graphical_lasso_.precision_similarity(mdl.get_observed_precision(), psi)\n",
    "        labels_pred = clust.set_params(n_clusters=np.unique(labels_true).size).fit_predict(tgl_prec_sims)\n",
    "        labels_pred = np.repeat(labels_pred, i)\n",
    "        tac = time() - tic\n",
    "        obs_precs = np.array([mdl.get_observed_precision()[l] for l in y])\n",
    "        obs_precs_sparse = np.array([mdl.get_precision()[l] for l in y])\n",
    "        \n",
    "        set_results(vs, mdl, 'ktgl', i, labels_true, labels_pred, \n",
    "                    thetas_true_sparse, thetas_true, obs_precs_sparse, obs_precs, tac)\n",
    "        \n",
    "        \n",
    "        # STGL\n",
    "        pipe_stgl = Pipeline([('stgl', clone(stgl)), ('clust', clone(clust))])\n",
    "        pipe_stgl.set_params(stgl__n_clusters=np.unique(labels_true).size,\n",
    "                             clust__n_clusters=np.unique(labels_true).size)\n",
    "        tic = time()\n",
    "        labels_pred = pipe_stgl.fit_predict(X, y)\n",
    "        tac = time() - tic\n",
    "        labels_pred = np.repeat(labels_pred, i)\n",
    "        \n",
    "        mdl = pipe_stgl['stgl']\n",
    "        obs_precs = np.array([mdl.get_observed_precision()[l] for l in y])\n",
    "        obs_precs_sparse = np.array([mdl.get_precision()[l] for l in y])\n",
    "        \n",
    "        set_results(vs, pipe_stgl, 'stgl', i, labels_true, labels_pred, \n",
    "                    thetas_true_sparse, thetas_true, obs_precs_sparse, obs_precs, tac)\n",
    "        \n",
    "        # GraphicalLasso + similarity\n",
    "        gl = GraphicalLasso(alpha=alpha)\n",
    "        tic = time()\n",
    "        mdl = [clone(gl).fit(X[y == c]) for c in np.unique(y)]\n",
    "        tac = time() - tic\n",
    "        precisions = np.array([m.precision_ for m in mdl])\n",
    "        psi = kernel_time_graphical_lasso_.check_norm_prox('l1')[0]\n",
    "        prec_sims = kernel_time_graphical_lasso_.precision_similarity(precisions, psi)\n",
    "        labels_pred = clust.set_params(n_clusters=np.unique(labels_true).size).fit_predict(prec_sims)\n",
    "        labels_pred = np.repeat(labels_pred, i)\n",
    "        obs_precs = np.array([precisions[l] for l in y])\n",
    "        obs_precs_sparse = np.array([precisions[l] for l in y])\n",
    "        \n",
    "        set_results(vs, mdl, 'gl', i, labels_true, labels_pred, \n",
    "                    thetas_true_sparse, thetas_true, obs_precs_sparse, obs_precs, tac)\n",
    "        \n",
    "        # TGL + similarity\n",
    "        tgl = time_graphical_lasso_.TimeGraphicalLasso(alpha=alpha, beta=beta, max_iter=500, psi='l1')\n",
    "        tic = time()\n",
    "        tgl.fit(X, y)        \n",
    "        tac = time() - tic\n",
    "        psi = kernel_time_graphical_lasso_.check_norm_prox(tgl.psi)[0]\n",
    "        tgl_prec_sims = kernel_time_graphical_lasso_.precision_similarity(tgl.get_observed_precision(), psi)\n",
    "        labels_pred = clust.set_params(n_clusters=np.unique(labels_true).size).fit_predict(tgl_prec_sims)\n",
    "        labels_pred = np.repeat(labels_pred, i)\n",
    "        mdl = tgl\n",
    "        obs_precs = np.array([mdl.get_observed_precision()[l] for l in y])\n",
    "        obs_precs_sparse = np.array([mdl.get_precision()[l] for l in y])\n",
    "        \n",
    "        set_results(vs, mdl, 'tgl', i, labels_true, labels_pred, \n",
    "                    thetas_true_sparse, thetas_true, obs_precs_sparse, obs_precs, tac)\n",
    "\n",
    "    \n",
    "        ticc = TICC_solver.TICC(number_of_clusters=np.unique(labels_true).size, window_size=1,\n",
    "                                lambda_parameter=alpha)\n",
    "        tic = time()\n",
    "        try:\n",
    "            ticc.fit(X)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"ticc error. %s\" % str(e), end=' ')\n",
    "            ticc.labels_ = np.zeros_like(labels_true)\n",
    "            ticc.precision_ = {0: np.eye(X.shape[1])}\n",
    "            \n",
    "        tac = time() - tic\n",
    "        \n",
    "        obs_precs = np.array([ticc.precision_[l] for l in ticc.labels_])\n",
    "        obs_precs_sparse = obs_precs\n",
    "        \n",
    "        set_results(vs, ticc, 'ticc', i, labels_true, ticc.labels_, \n",
    "                    thetas_true_sparse, thetas_true, obs_precs_sparse, obs_precs, tac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regain import utils\n",
    "# utils.save_pickle(datas, 'datas_1308_noshuffle_100.pkl')\n",
    "utils.save_pickle(vs, 'results_representative_1308_100_noshuffle.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) and (b) experiment\n",
    "shuffle = True\n",
    "\n",
    "np.random.seed(42)\n",
    "vs, ne, se = {}, {}, {}\n",
    "datas = {}\n",
    "for i in partitions:\n",
    "    for j in tqdm(range(n_splits)):\n",
    "        np.random.seed((i+j+3)*n_dim)\n",
    "        n_clusts = np.random.randint(10) + 5\n",
    "        data = kernels.make_cluster_representative(\n",
    "            n_clusters=n_clusts, n_dim=n_dim, n_samples=i, shuffle=shuffle)\n",
    "        X, y = data.X, data.y\n",
    "        n_times = np.unique(y).size\n",
    "        thetas_true = np.array([data.precs[l] for l in y])\n",
    "        thetas_true_sparse = np.array([data.sparse_precs[l] for l in y])\n",
    "        labels_true = data.id_cluster #[::(len(clusters) * i // n_times)]\n",
    "\n",
    "        \n",
    "        # WP + similarity\n",
    "        wp = wishart_process_.WishartProcess(kernel='ess', verbose=2, learn_ell=False,\n",
    "                                            n_iter=200).fit(X, y)\n",
    "        tic = time()\n",
    "        wp.fit(X, y)        \n",
    "        tac = time() - tic\n",
    "        tgl_prec_sims = kernel_time_graphical_lasso_.precision_similarity(wp.precision_, psi)\n",
    "        labels_pred = clust.set_params(n_clusters=np.unique(labels_true).size).fit_predict(tgl_prec_sims)\n",
    "        labels_pred = np.repeat(labels_pred, i)\n",
    "        mdl = wp\n",
    "        obs_precs = np.array([mdl.precision_[l] for l in y])\n",
    "        obs_precs_sparse = np.array([mdl.precision_[l] for l in y])\n",
    "        \n",
    "        set_results(vs, mdl, 'wp', i, labels_true, labels_pred, \n",
    "                    thetas_true_sparse, thetas_true, obs_precs_sparse, obs_precs, tac)\n",
    "        \n",
    "        \n",
    "        scales = np.unique(np.diff(np.where(data.cluster_series == 0)))\n",
    "        scales = scales[scales > 1]\n",
    "        kernel = RBF(1)\n",
    "        for s in scales:\n",
    "            kernel += ExpSineSquared(periodicity=s, length_scale=0.5) * 10\n",
    "        kern = kernel(np.arange(np.unique(y).size)[:, None])\n",
    "        \n",
    "        # KTGL + similarity\n",
    "        mdl = kernel_time_graphical_lasso_.KernelTimeGraphicalLasso(\n",
    "            alpha=1, max_iter=500, psi='l1', kernel=kern)\n",
    "        \n",
    "        tic = time()\n",
    "        mdl.fit(X, y)        \n",
    "        psi = kernel_time_graphical_lasso_.check_norm_prox(mdl.psi)[0]\n",
    "        tgl_prec_sims = kernel_time_graphical_lasso_.precision_similarity(mdl.get_observed_precision(), psi)\n",
    "        labels_pred = clust.set_params(n_clusters=np.unique(labels_true).size).fit_predict(tgl_prec_sims)\n",
    "        labels_pred = np.repeat(labels_pred, i)\n",
    "        tac = time() - tic\n",
    "        obs_precs = np.array([mdl.get_observed_precision()[l] for l in y])\n",
    "        obs_precs_sparse = np.array([mdl.get_precision()[l] for l in y])\n",
    "        \n",
    "        set_results(vs, mdl, 'ktgl', i, labels_true, labels_pred, \n",
    "                    thetas_true_sparse, thetas_true, obs_precs_sparse, obs_precs, tac)\n",
    "        \n",
    "        \n",
    "        # STGL\n",
    "        pipe_stgl = Pipeline([('stgl', clone(stgl)), ('clust', clone(clust))])\n",
    "        pipe_stgl.set_params(stgl__n_clusters=np.unique(labels_true).size,\n",
    "                             clust__n_clusters=np.unique(labels_true).size)\n",
    "        tic = time()\n",
    "        labels_pred = pipe_stgl.fit_predict(X, y)\n",
    "        tac = time() - tic\n",
    "        labels_pred = np.repeat(labels_pred, i)\n",
    "        \n",
    "        mdl = pipe_stgl['stgl']\n",
    "        obs_precs = np.array([mdl.get_observed_precision()[l] for l in y])\n",
    "        obs_precs_sparse = np.array([mdl.get_precision()[l] for l in y])\n",
    "        \n",
    "        set_results(vs, pipe_stgl, 'stgl', i, labels_true, labels_pred, \n",
    "                    thetas_true_sparse, thetas_true, obs_precs_sparse, obs_precs, tac)\n",
    "        \n",
    "        # GraphicalLasso + similarity\n",
    "        gl = GraphicalLasso(alpha=alpha)\n",
    "        tic = time()\n",
    "        mdl = [clone(gl).fit(X[y == c]) for c in np.unique(y)]\n",
    "        tac = time() - tic\n",
    "        precisions = np.array([m.precision_ for m in mdl])\n",
    "        psi = kernel_time_graphical_lasso_.check_norm_prox('l1')[0]\n",
    "        prec_sims = kernel_time_graphical_lasso_.precision_similarity(precisions, psi)\n",
    "        labels_pred = clust.set_params(n_clusters=np.unique(labels_true).size).fit_predict(prec_sims)\n",
    "        labels_pred = np.repeat(labels_pred, i)\n",
    "        obs_precs = np.array([precisions[l] for l in y])\n",
    "        obs_precs_sparse = np.array([precisions[l] for l in y])\n",
    "        \n",
    "        set_results(vs, mdl, 'gl', i, labels_true, labels_pred, \n",
    "                    thetas_true_sparse, thetas_true, obs_precs_sparse, obs_precs, tac)\n",
    "        \n",
    "        # TGL + similarity\n",
    "        tgl = time_graphical_lasso_.TimeGraphicalLasso(alpha=alpha, beta=beta, max_iter=500, psi='l1')\n",
    "        tic = time()\n",
    "        tgl.fit(X, y)        \n",
    "        tac = time() - tic\n",
    "        psi = kernel_time_graphical_lasso_.check_norm_prox(tgl.psi)[0]\n",
    "        tgl_prec_sims = kernel_time_graphical_lasso_.precision_similarity(tgl.get_observed_precision(), psi)\n",
    "        labels_pred = clust.set_params(n_clusters=np.unique(labels_true).size).fit_predict(tgl_prec_sims)\n",
    "        labels_pred = np.repeat(labels_pred, i)\n",
    "        mdl = tgl\n",
    "        obs_precs = np.array([mdl.get_observed_precision()[l] for l in y])\n",
    "        obs_precs_sparse = np.array([mdl.get_precision()[l] for l in y])\n",
    "        \n",
    "        set_results(vs, mdl, 'tgl', i, labels_true, labels_pred, \n",
    "                    thetas_true_sparse, thetas_true, obs_precs_sparse, obs_precs, tac)\n",
    "\n",
    "    \n",
    "        ticc = TICC_solver.TICC(number_of_clusters=np.unique(labels_true).size, window_size=1,\n",
    "                                lambda_parameter=alpha)\n",
    "        tic = time()\n",
    "        try:\n",
    "            ticc.fit(X)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"ticc error. %s\" % str(e), end=' ')\n",
    "            ticc.labels_ = np.zeros_like(labels_true)\n",
    "            ticc.precision_ = {0: np.eye(X.shape[1])}\n",
    "            \n",
    "        tac = time() - tic\n",
    "        \n",
    "        obs_precs = np.array([ticc.precision_[l] for l in ticc.labels_])\n",
    "        obs_precs_sparse = obs_precs\n",
    "        \n",
    "        set_results(vs, ticc, 'ticc', i, labels_true, ticc.labels_, \n",
    "                    thetas_true_sparse, thetas_true, obs_precs_sparse, obs_precs, tac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regain import utils\n",
    "# utils.save_pickle(datas, 'datas_1308_noshuffle_100.pkl')\n",
    "utils.save_pickle(vs, 'results_representative_1308_100_shuffle.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) experiment - LATENT\n",
    "\n",
    "np.random.seed(42)\n",
    "vs, ne, se = {}, {}, {}\n",
    "datas = {}\n",
    "for i in partitions[::-1]:\n",
    "    for j in (range(n_splits)):\n",
    "        np.random.seed((i+j+3)*n_dim)\n",
    "        n_clusts = np.random.randint(10) + 5\n",
    "        \n",
    "        data = kernels.make_ticc_dataset_v3(\n",
    "            clusters=np.random.choice(list(range(n_clusts)), size=20),\n",
    "            w_size=w_size, n_dim=n_dim, n_samples=i)\n",
    "        X, y = data.X, data.y\n",
    "        n_times = np.unique(y).size\n",
    "        thetas_true = np.array([data.precs[l] for l in y])\n",
    "        thetas_true_sparse = np.array([data.sparse_precs[l] for l in y])\n",
    "        labels_true = data.id_cluster #[::(len(clusters) * i // n_times)]\n",
    "\n",
    "        scales = np.unique(np.diff(np.where(data.id_cluster == 0)))\n",
    "        scales = scales[scales > 1]\n",
    "        kernel = RBF(1)\n",
    "        for s in scales:\n",
    "            kernel += ExpSineSquared(periodicity=s, length_scale=0.5) * 10\n",
    "        kern = kernel(np.arange(np.unique(y).size)[:, None])\n",
    "        \n",
    "        kern_phi = np.eye(np.unique(y).size)\n",
    "        np.fill_diagonal(kern_phi[:, 1:], eta)\n",
    "        np.fill_diagonal(kern_phi[1:], eta)\n",
    "        \n",
    "        # KLTGL + similarity\n",
    "        mdl = kernel_latent_time_graphical_lasso_.KernelLatentTimeGraphicalLasso(\n",
    "            alpha=alpha, tau=5, max_iter=500, psi='l1',\n",
    "            kernel_psi=kern,\n",
    "            kernel_phi=kern_phi)\n",
    "        tic = time()\n",
    "        mdl.fit(X, y)\n",
    "        psi = kernel_latent_time_graphical_lasso_.check_norm_prox(mdl.psi)[0]\n",
    "        ltgl_prec_sims = kernel_time_graphical_lasso_.precision_similarity(mdl.get_observed_precision(), psi)\n",
    "        labels_pred = clust.set_params(n_clusters=np.unique(labels_true).size).fit_predict(ltgl_prec_sims)\n",
    "        labels_pred = np.repeat(labels_pred, i)\n",
    "        tac = time() - tic\n",
    "        \n",
    "        obs_precs = np.array([mdl.get_observed_precision()[l] for l in y])\n",
    "        obs_precs_sparse = np.array([mdl.get_precision()[l] for l in y])\n",
    "        \n",
    "        set_results(vs, mdl, 'kltgl', i, labels_true, labels_pred, \n",
    "                    thetas_true_sparse, thetas_true, obs_precs_sparse, obs_precs, tac)\n",
    "\n",
    " \n",
    "        # SLTGL\n",
    "        pipe_sltgl = Pipeline([('sltgl', clone(sltgl)), ('clust', clone(clust))])\n",
    "        pipe_sltgl.set_params(sltgl__n_clusters=np.unique(labels_true).size,\n",
    "                              sltgl__tau=5,\n",
    "                              clust__n_clusters=np.unique(labels_true).size)\n",
    "        tic = time()\n",
    "        labels_pred = pipe_sltgl.fit_predict(X, y)\n",
    "        tac = time() - tic\n",
    "        labels_pred = np.repeat(labels_pred, i)\n",
    "        \n",
    "        mdl = pipe_sltgl['sltgl']\n",
    "        obs_precs = np.array([mdl.get_observed_precision()[l] for l in y])\n",
    "        obs_precs_sparse = np.array([mdl.get_precision()[l] for l in y])\n",
    "        \n",
    "        set_results(vs, pipe_sltgl, 'sltgl', i, labels_true, labels_pred, \n",
    "                    thetas_true_sparse, thetas_true, obs_precs_sparse, obs_precs, tac)\n",
    "        \n",
    "        \n",
    "        # LTGL + similarity\n",
    "        mdl = latent_time_graphical_lasso_.LatentTimeGraphicalLasso(\n",
    "            alpha=alpha, beta=beta, tau=5, eta=eta, max_iter=500, psi='l1')\n",
    "        tic = time()\n",
    "        mdl.fit(X, y)\n",
    "        tac = time() - tic\n",
    "        psi = kernel_latent_time_graphical_lasso_.check_norm_prox(mdl.psi)[0]\n",
    "        ltgl_prec_sims = kernel_time_graphical_lasso_.precision_similarity(mdl.get_observed_precision(), psi)\n",
    "        labels_pred = clust.set_params(n_clusters=np.unique(labels_true).size).fit_predict(ltgl_prec_sims)\n",
    "        labels_pred = np.repeat(labels_pred, i)\n",
    "        \n",
    "        obs_precs = np.array([mdl.get_observed_precision()[l] for l in y])\n",
    "        obs_precs_sparse = np.array([mdl.get_precision()[l] for l in y])\n",
    "        \n",
    "        set_results(vs, mdl, 'ltgl', i, labels_true, labels_pred, \n",
    "                    thetas_true_sparse, thetas_true, obs_precs_sparse, obs_precs, tac)\n",
    "                \n",
    "#         # TICC\n",
    "#         ticc = TICC_solver.TICC(number_of_clusters=np.unique(labels_true).size, window_size=1,\n",
    "#                                 lambda_parameter=alpha)\n",
    "#         tic = time()\n",
    "#         try:\n",
    "#             ticc.fit(X)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(\"ticc error. %s\" % str(e), end=' ')\n",
    "#             ticc.labels_ = np.zeros_like(labels_true)\n",
    "#             ticc.precision_ = {0: np.eye(X.shape[1])}\n",
    "            \n",
    "#         tac = time() - tic\n",
    "        \n",
    "#         obs_precs = np.array([ticc.precision_[l] for l in ticc.labels_])\n",
    "#         obs_precs_sparse = obs_precs\n",
    "        \n",
    "#         set_results(vs, ticc, 'ticc', i, labels_true, ticc.labels_, \n",
    "#                     thetas_true_sparse, thetas_true, obs_precs_sparse, obs_precs, tac)\n",
    "    from regain import utils\n",
    "    # utils.save_pickle(datas, 'datas_1308_noshuffle_100.pkl')\n",
    "    utils.save_pickle(vs, 'results_representative_1508_100_ticc.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import local_utils; reload(local_utils)\n",
    "from local_utils import *\n",
    "\n",
    "from regain import utils\n",
    "vs_0 = utils.load_pickle('/Users/federicot/Downloads/results_representative_1308_100_noshuffle.pkl')\n",
    "vs_1 = utils.load_pickle('/Users/federicot/Downloads/results_representative_1308_100_shuffle.pkl')\n",
    "vs_2 = utils.load_pickle('/Users/federicot/Downloads/results_representative_1508_100_ticc.pkl')\n",
    "\n",
    "vss = [vs_0, vs_1, vs_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_in_plot = ['time']\n",
    "in_plot = only_in_plot + ['mcc', 'error_norm', 'v_measure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_latent = False\n",
    "latent_mdls = ['KLTGL', 'SLTGL', 'LTGL', 'TICC']\n",
    "\n",
    "other_measures = ['time', 'error_norm', 'v_measure']\n",
    "mm = ['balanced_accuracy', 'average_precision', 'mcc'] + other_measures\n",
    "mm = np.array(mm).reshape(2, 3)\n",
    "mapping_names = dict(zip(['KTGL', 'STGL', 'KLTGL', 'SLTGL'], ['TGL$_\\kappa$', 'TGL$_P$', 'LTGL$_\\kappa$', 'LTGL$_P$']))\n",
    "    \n",
    "melt_exps = []\n",
    "for it, vs in enumerate(vss):\n",
    "    new_r = convert_dict_to_df(vs, max_samples=1000)\n",
    "    try:\n",
    "        ensure_ticc_valid(new_r)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    melt_0 = pd.melt(new_r.reset_index(), id_vars=['method', 'samples'], value_vars=['structure_error'])\n",
    "    for k in melt_0.value[0]:\n",
    "        melt_0[k] = melt_0.value.apply(lambda x: x[k] if x else np.nan)\n",
    "\n",
    "    melts = [melt_0] + [\n",
    "        pd.melt(new_r.reset_index(), id_vars=['method', 'samples'], value_vars=[val], value_name=val)\n",
    "        for val in other_measures\n",
    "    ]\n",
    "    melts = [\n",
    "        melt_.set_index(['method', 'samples', melt_.groupby(['method', 'samples']).cumcount()])\n",
    "        for melt_ in melts\n",
    "    ]\n",
    "\n",
    "    melt = pd.concat(melts, axis=1).sort_index(level=2).reset_index(level=2, drop=True).reset_index()\n",
    "\n",
    "    if not print_latent and it < 2:\n",
    "        melt = melt[~melt.method.isin(latent_mdls + ['ECOV'])]\n",
    "    else:\n",
    "        melt = melt[melt.method.isin(latent_mdls + ['TICC'])]\n",
    "    \n",
    "    melt.method = pd.Categorical(melt.method, [\"GL\", 'WP', \"TGL\", 'LTGL', 'KTGL', 'KLTGL', 'STGL',  'SLTGL'])\n",
    "    melt = melt.sort_values('method')\n",
    "    melt = melt.applymap(lambda x: x if not isinstance(x, str) else mapping_names.get(x, x))\n",
    "    melt_exps.append(melt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp, melt in zip(['noshuffle', 'shuffle', 'ticc'], melt_exps):\n",
    "    res_table = pd.DataFrame()\n",
    "    for m in mm.flatten():\n",
    "        if m in only_in_plot: continue\n",
    "    #     res_table[m] = melt[m].apply(format_3f)\n",
    "    #     res_table[m] = melt.groupby('method')[m].apply(format_3f)\n",
    "        res_table[m] = melt.groupby(['method', 'samples'][::-1])[m].apply(format_3f)\n",
    "\n",
    "    res_table[''] = np.where(res_table.index.levels[1].str.endswith('TGL$_\\kappa$'),\n",
    "                             '\\\\ldelim\\\\{{2}{5.5mm}[Ours]', '').tolist() * res_table.index.levels[0].size\n",
    "    res_table = res_table.reset_index().set_index(['samples', '', 'method'])\n",
    "\n",
    "    display(res_table.style.apply(highlight_max_std, axis=None))\n",
    "    \n",
    "    res_table.loc[idx[50, :]].applymap(lambda x: '$%s$'%x).to_latex(\n",
    "        \"results_100_50s_{}.tex\".format(exp), escape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = dict(zip([\"TGL\", 'TGL$_\\kappa$', 'TGL$_P$', \"GL\", 'WP'], sns.color_palette()))\n",
    "palette.update(dict(zip(['LTGL', 'LTGL$_\\kappa$', 'LTGL$_P$'], sns.color_palette())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt_idx = melt.set_index(['method', 'samples'])\n",
    "# for m in mm.flatten():\n",
    "#     melt_idx[m] -= melt_idx.loc[idx['TGL', :], m].groupby('samples').mean()\n",
    "\n",
    "plt.close('all')\n",
    "hh, ll = [], []\n",
    "f, ax = plt.subplots(len(melt_exps), len(in_plot), figsize=(16,7),\n",
    "                     sharex=True, sharey=False, squeeze=False)#, gridspec_kw=dict(wspace=0.4, hspace=0.1))\n",
    "\n",
    "for c, measure in enumerate(in_plot):\n",
    "    for r, melt in enumerate(melt_exps):\n",
    "        ax[r,c] = sns.pointplot(\n",
    "            x='samples', y=measure, data=melt,\n",
    "            hue='method', ax=ax[r,c], legend=False, palette=palette)\n",
    "        if c < len(in_plot) - 1:\n",
    "            ax[r,c].legend_ = None\n",
    "        else:\n",
    "            lgd = ax[r,c].legend(title=\"Exp ({})\".format(dict(zip(range(len(melt_exps)), list('abc')))[r]),\n",
    "                                 bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "            ll.append(lgd)\n",
    "        \n",
    "        if r < len(melt_exps) - 1:\n",
    "            ax[r,c].set_xlabel('')\n",
    "        if r == 0:\n",
    "            ax[r,c].set_title(\n",
    "                dict(zip(in_plot, ['Time', 'MCC', 'MSE', 'V-measure']))[measure]\n",
    "            )\n",
    "        ax[r,c].set_ylabel('')\n",
    "        \n",
    "        # ax[r,c].axhline(0, c='k', ls='--')\n",
    "        if measure in ['tp','fp','fn','tn', 'time', 'error_norm']:\n",
    "            ax[r,c].set_yscale('log')\n",
    "\n",
    "# for handles, labels in zip(hh, ll):\n",
    "#     lgd = f.legend(handles, labels, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.savefig('/Users/federicot/src/fdtomasi/wsdm20/images/plot_1308_all.pdf', \n",
    "          transparent=True, bbox_extra_artists=ll, bbox_inches='tight', dpi=600)\n",
    "#           transparent=True, bbox_extra_artists=(lgd,), bbox_inches='tight', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt_idx = melt.set_index(['method', 'samples'])\n",
    "# for m in mm.flatten():\n",
    "#     melt_idx[m] -= melt_idx.loc[idx['TGL', :], m].groupby('samples').mean()\n",
    "\n",
    "plt.close('all')\n",
    "f, ax = plt.subplots(mm.shape[0], mm.shape[1], figsize=(20,5), sharex=True, squeeze=False)\n",
    "for r in range(mm.shape[0]):\n",
    "    for c in range(mm.shape[1]):\n",
    "        ax[r,c] = sns.pointplot(\n",
    "            x='samples', y=mm[r,c], data=melt,\n",
    "            hue='method', ax=ax[r,c])\n",
    "        melt[mm[r,c]] = melt[mm[r,c]].astype(float)\n",
    "        \n",
    "        # ax[r,c].axhline(0, c='k', ls='--')\n",
    "        if mm[r,c] in ['tp','fp','fn','tn', 'time', 'error_norm']:\n",
    "            ax[r,c].set_yscale('log')\n",
    "    \n",
    "f.tight_layout()\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.savefig('plot_1308_shuffle.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vss = {}\n",
    "for k in vs:\n",
    "    if k[1] < 1000:\n",
    "        vss[k] = vs[k]\n",
    "\n",
    "neww = {}\n",
    "for k, v in vss.items():\n",
    "    new = {}\n",
    "    for kk, vv in v.items():\n",
    "        new.update({(kk, i): vvv for i, vvv in enumerate(vv)})\n",
    "    neww[k] = new\n",
    "\n",
    "res_df = pd.DataFrame(neww)\n",
    "res_df.index.name = ('measure', 'iter')\n",
    "\n",
    "rr = res_df.T.reset_index()\n",
    "rr = rr.rename(columns={'level_0': 'method', 'level_1': 'samples'})\n",
    "rr.method = rr.method.str.upper()\n",
    "\n",
    "new_r = rr.set_index(['method', 'samples'])\n",
    "\n",
    "for i in range(10):\n",
    "    new_r['valid', i] = True\n",
    "\n",
    "for r in new_r.loc['TICC', 'model'].iterrows():\n",
    "    sampl = r[0]\n",
    "    for k, rrr in (r[1].iteritems()):\n",
    "        if np.alltrue(rrr.labels_ == np.zeros_like(rrr.labels_)):\n",
    "            new_r.loc[('TICC', sampl), ('structure_error', k)] = None\n",
    "\n",
    "rr = new_r.reset_index()\n",
    "\n",
    "latent_mdls = ['KLTGL', 'SLTGL', 'LTGL']#, 'TICC']\n",
    "print_latent = False\n",
    "\n",
    "melt = pd.melt(rr, id_vars=['method', 'samples'], value_vars=['structure_error'])\n",
    "\n",
    "for k in melt.value[0]:\n",
    "    melt[k] = melt.value.apply(lambda x: x[k] if x else np.nan)\n",
    "\n",
    "# mm = np.array(list(melt.value[0].keys())).reshape(7,3)\n",
    "mm = np.array(['balanced_accuracy', 'average_precision', 'mcc'])\n",
    "\n",
    "if not print_latent:\n",
    "    melt = melt[~melt.method.isin(latent_mdls + ['ECOV'])]\n",
    "else:\n",
    "    melt = melt[melt.method.isin(latent_mdls + ['TICC'])]\n",
    "\n",
    "melt_idx = melt.set_index(['method', 'samples'])\n",
    "# for m in mm.flatten():\n",
    "#     melt_idx[m] -= melt_idx.loc[idx['TGL', :], m].groupby('samples').mean()\n",
    "\n",
    "res_table = pd.DataFrame()\n",
    "for m in mm.flatten():\n",
    "    res_table[m] = melt.groupby('method')[m].apply(format_3f)\n",
    "\n",
    "# plt.close('all')\n",
    "# f, ax = plt.subplots(mm.shape[0], mm.shape[1], figsize=(20,10), sharex=True)\n",
    "# for r in range(mm.shape[0]):\n",
    "#     for c in range(mm.shape[1]):\n",
    "#         ax[r,c] = sns.pointplot(x='samples', y=mm[r,c], data=melt_idx.reset_index(), hue='method', ax=ax[r,c])\n",
    "# #         ax[r,c].axhline(0, c='k', ls='--')\n",
    "#         if mm[r,c] in ['tp','fp','fn','tn']:\n",
    "#             ax[r,c].set_yscale('log')\n",
    "\n",
    "# f.tight_layout()\n",
    "# f\n",
    "# f.savefig('structure_error_no_shuffle.pdf')\n",
    "\n",
    "plt.close('all')\n",
    "f, ax = plt.subplots(1,3, figsize=(15,5), sharex=True)\n",
    "for i, val in enumerate(['time', 'error_norm', 'v_measure']):\n",
    "    melt = pd.melt(rr, id_vars=['method', 'samples'], value_vars=[val], value_name=val)\n",
    "    if not print_latent:\n",
    "        melt = melt[~melt.method.isin(latent_mdls + ['ECOV'])]\n",
    "    else:\n",
    "        melt = melt[melt.method.isin(latent_mdls + ['TICC'])]\n",
    "    res_t = melt.groupby('method')[val].apply(format_3f)\n",
    "    res_table[val] = res_t\n",
    "    \n",
    "    # melt = melt[~(melt.method.isin(['TICC',]) & melt.samples != 500)]\n",
    "    \n",
    "    melt_idx = melt.set_index(['method', 'samples'])\n",
    "    \n",
    "    melt_idx[val] = melt_idx[val].astype(float)\n",
    "#     melt_idx[val] -= melt_idx.loc[idx['TGL', :], val].groupby('samples').mean()\n",
    "    melt_idx.loc[idx['TICC', [5, 10, 50, 100]], val] = np.nan\n",
    "\n",
    "    ax[i] = sns.pointplot(x='samples', y=val, data=melt_idx.reset_index(), hue='method', ax=ax[i])\n",
    "    if val in ['time', 'error_norm']:\n",
    "        ax[i].set_yscale('log')\n",
    "f.tight_layout()\n",
    "\n",
    "res_table.style.apply(highlight_max_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enclose_num(x):\n",
    "    mean = x.split(' ')[0]\n",
    "    bm = False\n",
    "    if 'bm' in mean:\n",
    "        bm = True\n",
    "        mean = mean.split('{')[-1]\n",
    "    std = x.split(' ')[-1].split(')')[0]\n",
    "    r = '\\expn{%s}{%s} (\\pm \\expn{%s}{%s})' % (*mean.split('e'), *std.split('e'))\n",
    "    if bm:\n",
    "        return '\\\\bm{%s}' % r\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_table.applymap(enclose_num).applymap(lambda x: '$%s$'%x).to_latex(\"results_ticc.tex\", escape=False)\n",
    "res_table.applymap(lambda x: '$%s$'%x).to_latex(\"results_noshuffle_3f_5samples.tex\", escape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regain.plotting import results\n",
    "mdls = ['stgl', 'sltgl', 'tgl', 'ltgl', 'ticc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdls = np.unique([x[0] for x in vs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datas = utils.load_pickle('/Users/federicot/Downloads/datas_0508_shuffle.pkl')\n",
    "\n",
    "vs = utils.load_pickle('/Users/federicot/Downloads/results_representative_0508_ticc_2.pkl')\n",
    "\n",
    "# for f in ['/Users/federicot/Downloads/results_representative_0508_shuffle.pkl',]:\n",
    "# #           '/Users/federicot/Downloads/results_representative_0508_ticc.pkl']:\n",
    "#     vs.update(utils.load_pickle(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noshuffle = utils.load_pickle('/Users/federicot/Downloads/datas_0508_ticc_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, k1 in zip(datas, data_noshuffle):\n",
    "    datas[k]['data'] += data_noshuffle[k]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.DataFrame(index=product(['no_shuffle', 'shuffle'], datas.keys()), columns=range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trues = np.vstack(np.array([[d['precs'] for d in datas[k]['data']] for k in datas]))\n",
    "trues_noshuffle = np.vstack(np.array([[d['precs'] for d in data_noshuffle[k]['data']] for k in data_noshuffle]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trues = np.vstack((trues, trues_noshuffle))\n",
    "all_trues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = {}\n",
    "for mdl in mdls:\n",
    "    pr = []\n",
    "    vs[mdl] = [vs[(mdl, n)]['model'] for n in data_noshuffle]\n",
    "    if mdl == 'ticc': continue\n",
    "#         vs[mdl] = np.vstack(vs[mdl])\n",
    "#         for x in vs[mdl]:\n",
    "#             precision_ = [np.array([xx.precision_[l] for l in xx.labels_]) for xx in x]\n",
    "#             pr.append(np.array(np.abs(precision_)))\n",
    "            \n",
    "    if mdl in ['ecov', 'gl']:\n",
    "        for x in vs[mdl]:\n",
    "            prr = []\n",
    "            for xx in x:\n",
    "                precision_ = np.array([xxx.precision_ for xxx in xx])\n",
    "                prr.append(np.array(np.abs(precision_)))\n",
    "            pr.append(prr)\n",
    "            \n",
    "#         pp[mdl] = np.array([np.abs(np.array([xx.precision_ for xx in x]))\n",
    "#                    for x in vs[mdl]])\n",
    "    else:\n",
    "#             pr.append(np.array([xx.precision_ for xx in x]))\n",
    "        if not mdl.startswith('s'):\n",
    "            vs[mdl] = np.vstack(vs[mdl])\n",
    "            \n",
    "        for x in vs[mdl]:\n",
    "            if mdl.startswith('s'):\n",
    "                precision_ = [xx[mdl].precision_ for xx in x]\n",
    "            else:\n",
    "                precision_ = [xx.precision_ for xx in x]\n",
    "            pr.append(np.array(np.abs(precision_)))\n",
    "    \n",
    "    pr = np.vstack(np.array(pr))\n",
    "    pr[pr < 1e-3] = 0\n",
    "    pp[mdl] = pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "f, ax = plt.subplots(1, 1, figsize=(20,5), sharex=True)\n",
    "\n",
    "results.plot_curve(trues_noshuffle, pp, ax=ax, fontsize=11, mode='roc', multiple_true=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(results)\n",
    "plt.close('all')\n",
    "f, ax = plt.subplots(1, 1, figsize=(20,10), sharex=True)\n",
    "\n",
    "for i, n in enumerate(res_df.columns.levels[1]):\n",
    "    pp = {}\n",
    "    tt = ([np.abs(x['precs']) for x in datas[n]['data']])\n",
    "    for mdl in mdls:\n",
    "#         print(len(pp), len(tt))\n",
    "        if mdl == 'ticc':\n",
    "            \n",
    "            obs_precs = np.array([ticc.precision_[l] for l in ticc.labels_])\n",
    "        if mdl in ['ecov', 'gl']:\n",
    "            pp[mdl] = [np.abs(np.array([xx.precision_ for xx in x]))\n",
    "                       for x in vs[(mdl, n)]['model']]\n",
    "        else:\n",
    "            pp[mdl] = [np.abs(x[mdl].precision_ if mdl.startswith('s') else x.precision_)\n",
    "                       for x in vs[(mdl, n)]['model']]\n",
    "    results.plot_curve(tt, pp, ax=ax[i], fontsize=11, mode='roc', multiple_true=True)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(results)\n",
    "plt.close('all')\n",
    "f, ax = plt.subplots(7, 1, figsize=(20,50), sharex=True)\n",
    "\n",
    "for i, n in enumerate(res_df.columns.levels[1]):\n",
    "    pp = {}\n",
    "    tt = ([np.abs(x['precs']) for x in datas[n]['data']])\n",
    "    for mdl in mdls:\n",
    "#         print(len(pp), len(tt))\n",
    "        if mdl == 'ticc': continue\n",
    "        if mdl in ['ecov', 'gl']:\n",
    "            pp[mdl] = [np.abs(np.array([xx.precision_ for xx in x]))\n",
    "                       for x in vs[(mdl, n)]['model']]\n",
    "        else:\n",
    "            pp[mdl] = [np.abs(x[mdl].precision_ if mdl.startswith('s') else x.precision_)\n",
    "                       for x in vs[(mdl, n)]['model']]\n",
    "    results.plot_curve(tt, pp, ax=ax[i], fontsize=11, mode='roc', multiple_true=True)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.savefig('roc.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
